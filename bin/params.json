{"name":"Scrapie","tagline":"Web Scraper","body":"![Scrapie](src/main/images/sheepVerySmall.png) Scrapie\r\n======= \r\nA Web Scraper.  \r\n_Not the fatal, degenerative disease that affects the nervous systems of sheep and goats._\r\n\r\n[ ![Codeship Status for groupby/scrapie](https://codeship.io/projects/1df14350-ef55-0131-b5ae-023491d184db/status)](https://codeship.io/projects/27011) [![Stories in Ready](https://badge.waffle.io/groupby/scrapie.png?label=ready&title=Ready)](https://waffle.io/groupby/scrapie)\r\nQuickstart\r\n------\r\n\r\n1. Make sure Java 1.7 is on your computer and the java command is on your path.\r\n1. Download the [scrapie-latest.zip](scrapie-latest.zip?raw=true) \r\n1. Unpack it `scrapie-x.x.x` and go into the directory `cd scrapie-x.x.x`\r\n1. run the test \r\n - on *nix `./scrapie -f google.js -o google.xml`   \r\n - on windows `scrapie.bat -f google.js -o google.xml` \r\n\r\nDocs\r\n----\r\n\r\nRead the [API.md](API.md) and the [Emitter.md](Emitter.md) to learn about scrapie.\r\n\r\nUsage\r\n-----\r\n\r\nRequires that Java 1.7 is installed and on your path.\r\n\r\n```\r\nusage: scrapie\r\n -f,--file <arg>        The JavaScript file to use\r\n -o,--output <arg>      The file to output to\r\n -r,--record <arg>      Record this run and stop after N records have been emitted\r\n -t,--type <arg>        The record type, json or xml (default)\r\n -v,--verbosity <arg>   Log Level, trace, debug, info (default)\r\n```\r\n\r\n    ./scrapie -f myScraper.js -o records.xml\r\n\r\nExamples Scraper Files\r\n------\r\n\r\n\r\n###Low Complexity\r\n\r\nWhere each URL contains one record.\r\n\r\n```JavaScript\r\nvar urlIterator = new UrlIterator(function(pIndex){\r\n    if (pIndex < 2) {\r\n\t\treturn \"http://www.example.com/index.html?id=\" + pIndex;\r\n\t } else {\r\n\t\treturn null;\r\n\t }\r\n});\r\nurlIterator.forEach(function(pContext){\r\n   pContext.emit(\"title\", pContext.getJqText(\"title\"));\r\n   pContext.flush();\r\n});\r\n```\r\n\r\n###Medium Complexity\r\n\r\nWhere each URL is a list page with ten items and each item should be emitted as a separate record.\r\n\r\n```JavaScript\r\nvar urlIterator = new UrlIterator(function(pIndex){\r\n    if (pIndex < 2) {\r\n\t\treturn \"http://www.example.com/list?page=\" + pIndex;\r\n\t } else {\r\n\t\treturn null;\r\n\t }\r\n});\r\nurlIterator.forEach(function(pContext) {\r\n    pContext.breakIntoSections(\".item\", function(pContext){\r\n        process(pContext);\r\n        pContext.flush();\r\n    });\r\n});\r\nfunction process(pContext){\r\n \tvar id = pContext.getJq(\"a\").attr(\"href\").split(\"=\")[1];\r\n \tpContext.emit(\"id\", id);\r\n    pContext.emit(\"title\", pContext.getJqText(\"a\"));\r\n}\r\n\r\n```\r\n\r\n###High Complexity\r\n\r\nWhere each URL is a list page, each list page has 10 items and each item has a detail page URL with additional info.\r\n\r\n```JavaScript\r\nvar urlIterator = new UrlIterator(function(pIndex){\r\n    if (pIndex < 2) {\r\n\t\treturn \"http://www.example.com/list?page=\" + pIndex;\r\n\t } else {\r\n\t\treturn null;\r\n\t }\r\n});\r\nurlIterator.forEach(function(pContext) {\r\n\tprint(pContext.getHtml());\r\n    pContext.breakIntoSections(\".item\", function(pContext){\r\n     \tvar workingId = pContext.getJq(\"a\").attr(\"href\").split(\"=\")[1];\r\n    \tpContext.setWorkingId(workingId);\r\n        processListItem(pContext);\r\n     \tpContext.emitForWorkingId(\"id\", workingId);\r\n        pContext.processUrlsJq(\"a\", function(pContext){\r\n            processDetailPage(pContext);\r\n            pContext.flush();\r\n        });\r\n    });    \r\n});\r\nfunction processListItem(pContext){\r\n    pContext.emitForWorkingId(\"title\", pContext.getJqText(\"a\"));\r\n}\r\nfunction processDetailPage(pContext){\r\n    pContext.emitForWorkingId(\"price\", pContext.getJqText(\"#price\"));\r\n}\r\n```\r\n\r\nGoals\r\n-----\r\n\r\nA scraper that will generate URLs to crawl and convert them into objects we want to keep.\r\n\r\n- must not use XML configuration as using XML to parse HTML is an escaping nightmare.\r\n- must understand the concept of multiple of the same objects being created from one big page.\r\n- must be able to log in to password protected sites.\r\n- must be able to understand the concpet of a listing page that goes to a detail page to generate the object or objects.\r\n- must be able to resuse global items across pages.  Maybe back multiple pages.\r\n- the syntax must be as small as possible.\r\n- be threaded (which it isn't yet)\r\n\r\nChoices\r\n------\r\nUnder the hood the JavaScript scraper files connect to a Java object that uses Jsoup.  \r\nJsoup was extended to include XPath support.  \r\nA regular expression matcher is also available.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}